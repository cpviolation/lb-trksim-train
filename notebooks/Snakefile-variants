# Snakefile for training models and variants.
# prerequisite: preprocessing of all dataset
# execute with: snakemake -s Snakefile-variants --forceall --resource gpu=1 (--latency-wait 60)
# create DAG plot with: snakemake -s Snakefile-variants --dag | dot -Tpdf > dag-train.pdf
 
# directive for correct order of execution
ruleorder: validate_variant_model > validate_model > train_model > train_variant_model
#ruleorder: validate_model > validate_variant_model > train_variant_model > train_model
# dictionary holding the model's home directories and variant labels
configfile: "configs_db.yaml"

from glob import glob
from os import environ,path,makedirs
from copy import deepcopy
from pathlib import Path
from json import load
from pprint import pprint

conf = config.get('conf',"default")
if "yaml" in conf:
    environ['CONFIG_FILE'] = conf
else:
    environ['CONFIG_FILE'] = config['configurations'][conf]
configfile: environ['CONFIG_FILE']

TRAIN_DEFAULT_MODELS = True


# load definition of variant models and training folders
with open("Models_definitions.json") as mdl_file:
    models_def = load(mdl_file)

environ['DATA_STORAGE'] = "/mlinfn/minio/lhcb-data/anderlinil/LamarrBenderTrain"

models_collection = dict() # collect the info identical for all the models

for train_key in config['training_folder'].keys():
    models_collection[train_key] = dict()
    
    models_collection[train_key]['HOME_DIR']             = "/mlinfn/shared/lamarr/"+ environ['USERNAME']
    models_collection[train_key]['TRAINING_DATA_FOLDER'] = config['training_folder'][train_key]
    models_collection[train_key]['MODEL_STORAGE']        = models_collection[train_key]['HOME_DIR'] + "/trained_models" + models_collection[train_key]['TRAINING_DATA_FOLDER']
    models_collection[train_key]['FEATHER_FOLDER']       = models_collection[train_key]['HOME_DIR'] + \
                                                           "/lb-trksim-train/notebooks/feather_folder" + \
                                                           models_collection[train_key]['TRAINING_DATA_FOLDER']
# set empty dicts for missing config keys
config.setdefault('model_defaults',{})
config.setdefault('model_variants',{})

collection = dict()   # collect all the infos of all the models
#models_paths = dict() # collect all the model paths
models2train = list(config['model_defaults'].keys())+list(config['model_variants'].keys()) # get models that should be trained, either default or variants
models2train = list(set(models2train))

html_outputs = [] # list of all converted notebook to be obtained
# list of pairs of model,folder of the models to deploy
config.setdefault("deploy_models",{})
models2deploy = [(model,config['training_folder'][train_key]) for model in config['deploy_models'].keys() for train_key in config['deploy_models'][model]]
# NEED TO FINALIZE ORGANIZATION FOR DEPLOY SUBSET OF MODELS IN DIFFERENT TRAINING FOLDERS

for model in models2train: 
    collection[model] = dict(variants=[])
    
    for train_key in config['training_folder'].keys():
        collection[model][train_key] = dict(models_collection[train_key])
        
        collection[model][train_key]['TRAIN_DATA'      ] = collection[model][train_key]['FEATHER_FOLDER']+f"/{model}-train"
        collection[model][train_key]['VALIDATION_DATA' ] = collection[model][train_key]['FEATHER_FOLDER']+f"/{model}-validation"
        collection[model][train_key]['TEST_DATA'       ] = collection[model][train_key]['FEATHER_FOLDER']+f"/{model}-test"
        collection[model][train_key]['OUTPUT_MODEL'    ] = collection[model][train_key]['MODEL_STORAGE']+f"/models/{model}/saved_model.pb"
        collection[model][train_key]['INPUT_MODEL'     ] = collection[model][train_key]['MODEL_STORAGE']+f"/models/{model}/saved_model.pb"
        collection[model][train_key]['EXPORT_NOTEBOOK' ] = models_collection[train_key]['HOME_DIR'] + f"/notebooks_exports{config['training_folder'][train_key]}"
        collection[model][train_key]['MODEL_VARIANT'   ] = '' # default key is empty string
        collection[model][train_key]['MODEL_DEFINITION'] = models_def[model]['']
    
        # add default model
        if train_key in config['model_defaults'].get(model,[]):
            html_outputs.append(f"{collection[model][train_key]['EXPORT_NOTEBOOK']}/{model.capitalize()}-validation.html") # add default model
        
        # add variant models
        if len( config['model_variants'].get(model,{}).get(train_key,[]) ) > 0:
            for variant in config['model_variants'][model][train_key]:
                html_outputs.append(f"{collection[model][train_key]['EXPORT_NOTEBOOK']}/{model.capitalize()}_{variant}-validation.html")
                
# models paths with wildcards to parallelize training
nb_htmlpath_wildcards    = "/mlinfn/shared/lamarr/"+ environ['USERNAME'] + "/notebooks_exports/{folder}/{nbname}"
nbvar_htmlpath_wildcards = "/mlinfn/shared/lamarr/"+ environ['USERNAME'] + "/notebooks_exports/{folder}/{nbname}_{variant}"

# list of training folders
TRAIN_FOLDERS = [path for path in config['training_folder'].values()]

# needed to access environ variables from train_folder and not from train_key
inverse_map = {v: k for k, v in config['training_folder'].items()}

# match model notebook with preprocessing notebook
preprocessing_nb = dict(Acceptance="Preprocessing",Efficiency="Preprocessing",Resolution="Preprocessing-GANs",Covariance="Preprocessing-GANs")

# environment variables used by the notebooks
env_keys = ["HOME_DIR","TRAINING_DATA_FOLDER","MODEL_STORAGE","FEATHER_FOLDER", \
            "TRAIN_DATA","VALIDATION_DATA","TEST_DATA","OUTPUT_MODEL","INPUT_MODEL","MODEL_VARIANT","MODEL_DEFINITION"]

#the export to html is done by snakemake
environ['NB_EXPORT'] = "False"
                
rule all:
    input:
        validations = html_outputs,
        deploy = expand("reports{folder}/Deploy.html",folder=[folder for _,folder in models2deploy])
    output:
        "touched/all.done"
    message: "Trained these models: {input}"
    
    run:
        shell("touch {output}")
        

rule preprocessing:
    input:
        notebook="Preprocessing.ipynb",
        data_files= lambda wildcards: glob(environ['DATA_STORAGE']+f"/{wildcards.folder}/*.root")
       
    output:
        html="reports/{folder}/Preprocessing.html",
        
    message:
        "Preprocessing data in {wildcards.folder} for acceptance and efficiency models"
        
    resources:
        gpu=0
   
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        # update environ variables
        local_keys = ["HOME_DIR","TRAINING_DATA_FOLDER","MODEL_STORAGE","FEATHER_FOLDER"]
        environ.update({key: models_collection[inverse_map['/'+wildcards.folder]][key] for key in local_keys})
        environ['INPUT_FILES'] = " ".join(input.data_files)
        for m in ['acceptance','efficiency']:
            environ[f"{m.upper()}_PREPROCESSING"]   = environ['MODEL_STORAGE']+f"/{wildcards.folder}/models/{m}/tX.pkl"
            for f in ['train','test','validation']:
                environ[f"{m.upper()}_{f.upper()}"] = environ['FEATHER_FOLDER']+f"/{wildcards.folder}/{m}-{f}"
        
        shell("jupyter nbconvert --to html --execute --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=hep --no-input --no-prompt {input.notebook} --output {output.html} ")
        #shell("touch {output.html}")


rule preprocessing_gans:
    input:
        notebook="Preprocessing-GANs.ipynb",
        data_files= lambda wildcards: glob(environ['DATA_STORAGE']+f"/{wildcards.folder}/*.root")
    
    output:
        html="reports/{folder}/Preprocessing-GANs.html",

    message:
        "Preprocessing data in {wildcards.folder} for GAN resolution and covariance models"
        
    resources:
        gpu=0
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        # update environ variables
        local_keys = ["HOME_DIR","TRAINING_DATA_FOLDER","MODEL_STORAGE","FEATHER_FOLDER"]
        environ.update({key: models_collection[inverse_map['/'+wildcards.folder]][key] for key in local_keys})
        environ['INPUT_FILES'] = " ".join(input.data_files)
        for m in ['resolution','covariance']:
            environ[f"{m.upper()}_PREPROCESSING_X"]   = environ['MODEL_STORAGE']+f"/{wildcards.folder}/models/{m}/tX.pkl"
            environ[f"{m.upper()}_PREPROCESSING_Y"]   = environ['MODEL_STORAGE']+f"/{wildcards.folder}/models/{m}/tY.pkl"
            for f in ['train','test','validation']:
                environ[f"{m.upper()}_{f.upper()}"] = environ['FEATHER_FOLDER']+f"/{wildcards.folder}/{m}-{f}"
        
        shell("jupyter nbconvert --to html --execute --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=hep --no-input --no-prompt   {input.notebook} --output {output.html}")
        #shell("touch {output.html}")

        
rule train_variant_model:
    input:
        preprocessing = lambda wildcards: "reports/{folder}/"+f"{preprocessing_nb[wildcards.nbname]}.html",
        notebook      = lambda wildcards: f"{wildcards.nbname}.ipynb"
    output:
        html = nbvar_htmlpath_wildcards+".html"
    benchmark:
        "benchmarks/{folder}/{nbname}_{variant}.txt"
    resources:
        gpu=1
    message: "Training {wildcards.nbname} model (variant {wildcards.variant}) on {wildcards.folder} folder"
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        environ.update({key: collection[wildcards.nbname.lower()][inverse_map['/'+wildcards.folder]][key] for key in env_keys})
        
        shell("jupyter nbconvert --to HTML --execute {input.notebook} --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.kernel_name=hep --no-input --allow-errors --output {output.html}")
        #shell("touch {output.html}")
        

rule train_model:
    input:
        preprocessing = lambda wildcards: "reports/{folder}/"+f"{preprocessing_nb[wildcards.nbname]}.html",
        notebook      = lambda wildcards: f"{wildcards.nbname}.ipynb"
    output:
        html = nb_htmlpath_wildcards+".html"
    benchmark:
        "benchmarks/{folder}/{nbname}.txt"
    resources:
        gpu=1
    message: "Executing {input.notebook} to train {wildcards.nbname} model on {wildcards.folder} folder"
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        environ.update({key: collection[wildcards.nbname.lower()][inverse_map['/'+wildcards.folder]][key] for key in env_keys})
        
        shell("jupyter nbconvert --to HTML --execute {input.notebook} --ExecutePreprocessor.timeout=-1  --ExecutePreprocessor.kernel_name=hep --no-input --allow-errors --output {output.html}" )
        #shell("touch {output.html}")
        
        
rule validate_variant_model:
    input:
        html = nbvar_htmlpath_wildcards+".html"
    output:
        html = nbvar_htmlpath_wildcards+"-validation.html"
    params:
        notebook = lambda wildcards: f"{wildcards.nbname}-validation.ipynb"
    resources:
        gpu=1
    message:
        "Validate {wildcards.nbname} variant model trained on {wildcards.folder} folder"
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        environ.update({key: collection[wildcards.nbname.lower()][inverse_map['/'+wildcards.folder]][key] for key in env_keys})
        
        shell("jupyter nbconvert --to HTML --execute {params.notebook} --ExecutePreprocessor.timeout=-1  --ExecutePreprocessor.kernel_name=hep --no-input --allow-errors --output {output.html}" )
        #shell("touch {output.html}")
        
rule validate_model:
    input:
        html = nb_htmlpath_wildcards+".html"
    output:
        html = nb_htmlpath_wildcards+"-validation.html"
    params:
        notebook = lambda wildcards: f"{wildcards.nbname}-validation.ipynb"
    resources:
        gpu=1
    message:
        "Validate {wildcards.nbname} model trained on {wildcards.folder} folder"
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        environ.update({key: collection[wildcards.nbname.lower()][inverse_map['/'+wildcards.folder]][key] for key in env_keys})
        
        shell("jupyter nbconvert --to HTML --execute {params.notebook} --ExecutePreprocessor.timeout=-1  --ExecutePreprocessor.kernel_name=hep --no-input --allow-errors --output {output.html}" )
        shell("touch {output.html}")

rule deploy_model:
    input:
        html = lambda wildcards: [nb_htmlpath_wildcards.replace("/{folder}","{folder}").replace("{nbname}",model.capitalize())+"-validation.html" for model,folder in models2deploy]
    output:
        html= "reports{folder}/Deploy.html"
    resources:
        gpu=0
    message:
        "Deploy these validated models: {input.html}"
    
    run:
        makedirs(path.dirname(output.html),exist_ok=True) # create if not exists
        local_keys = ["HOME_DIR","TRAINING_DATA_FOLDER","MODEL_STORAGE","FEATHER_FOLDER"]
        environ.update({key: models_collection[inverse_map[wildcards.folder]][key] for key in local_keys})
        environ['GENERATED_C_FILE'] = "exported"+environ['TRAINING_DATA_FOLDER']+"/generated.C"
        for m in ['acceptance','efficiency','resolution','covariance']:
            environ[f"{m.upper()}_MODEL"]     = environ['MODEL_STORAGE']+f"/models/{m}/saved_model.pb"
            environ[f"{m.upper()}_TEST_DATA"] = environ['FEATHER_FOLDER']+f"/{m}-validation"
        
        shell("jupyter nbconvert --to HTML --execute Deploy.ipynb --ExecutePreprocessor.timeout=-1  --ExecutePreprocessor.kernel_name=hep --no-input --allow-errors --output {output.html}" )
        #shell("touch {output.html}")