{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining Environment variables pointing to user folders\n",
    "from os import environ,path\n",
    "\n",
    "environ.setdefault('HOME_DIR', \"/mlinfn/shared/lamarr/\"+ environ['USERNAME'])\n",
    "environ.setdefault('TRAINING_DATA_FOLDER',\"/j100\")\n",
    "environ.setdefault('MODEL_STORAGE' ,environ['HOME_DIR']+\"/trained_models\"+environ['TRAINING_DATA_FOLDER'])\n",
    "environ.setdefault('FEATHER_FOLDER',environ['HOME_DIR']+\"/lb-trksim-train/notebooks/feather_folder\"+environ['TRAINING_DATA_FOLDER'])\n",
    "environ.setdefault(\"TEST_DATA\"     ,environ['FEATHER_FOLDER']+\"/acceptance-test\")\n",
    "\n",
    "environ.setdefault(\"MODEL_VARIANT\",\"\")\n",
    "default_input_model = \"/models/acceptance/saved_model.pb\"\n",
    "if environ['MODEL_VARIANT'] != '':\n",
    "    model_path,model_name = path.split(default_input_model)\n",
    "    default_input_model   = path.join(model_path,environ['MODEL_VARIANT'],model_name)\n",
    "    \n",
    "environ.setdefault('INPUT_MODEL',environ['MODEL_STORAGE']+default_input_model)\n",
    "\n",
    "_ = environ.setdefault('NB_EXPORT',\"True\") # whether export notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the acceptance model\n",
    "##### Tested on environment `LHCb Analysis Facility` from [landerlini/lhcbaf:v0p8](https://hub.docker.com/r/landerlini/lhcbaf)\n",
    "\n",
    "This notebook is part of the pipeline to model the acceptance of charged particles in LHCb.\n",
    "In particular, it requires:\n",
    " * the preprocessing step, defined in the [Preprocessing.ipynb](./Preprocessing.ipynb) notebook\n",
    " * the training step, defined in the [Acceptance.ipynb](./Acceptance.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and environment\n",
    "In this notebook (and other validation notebooks) we will use GPUs to process the data and build the histograms.\n",
    "In most cases, the time is dominated by the graphics functions in `matplotlib` to the benefit from using GPUs is marginal, but this may change in the future when processing larger datasets.\n",
    "\n",
    "To process data with the GPU we are using [`cupy`](https://cupy.dev/) implementing a numpy-compatible library of numerical functions accelerated by a GPU, and [cudf](https://docs.rapids.ai/api/cudf/stable/) which is interfaced with [dask](https://docs.dask.org/en/stable/dataframe.html), enabling streaming data to the GPU while processing them with `cupy`. `cudf` is strictly needed only when the GPU memory is insufficient to cope with the whole dataset to be processed as it automates the process of splitting the dataset in batches, loading each batch to GPU, apply some processing with `cupy` retrieving and store the output to free the GPU memory, and then continue with another batch. \n",
    "\n",
    "With the current volume of data, this is not necessary, but again, we plan to increase the training datasets soon.\n",
    "\n",
    "> Unfortunately, `cudf` and `tensorflow` have inconsistent dependencies that, at the time of writing, make it impossible to have both of the libraries running on the GPU in the same environment. In the validation notebooks, where the most of the computing power is needed to split, sort and organize data in histograms, we will evaluate the model on CPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import environ\n",
    "\n",
    "## Remove annoying warnings \n",
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/envs/root/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as ddf\n",
    "import cudf\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We are now loading data in *Apache feather* format using our custom `FeatherReader`. In this case we will read the **Test** dataset as a \n",
    "`dask` dataframe.\n",
    "Note that the **Test** dataset was obtained from the overall dataset in the preprocessing step and was never loaded in the training notebook, so it can be considered completely independnet of both the training and validation datasets used to define weights and architecture of the neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from feather_io import FeatherReader\n",
    "data_reader = FeatherReader(environ.get(\"TEST_DATA\", \"acceptance-test\"))\n",
    "test_dataset = data_reader.as_dask_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessing step\n",
    "\n",
    "To produce plots of physically-meaningful variables, we need to invert and apply the preprocessing step to the data stored on disk.\n",
    "The preprocessing step was defined in the [`Preprocessing.ipynb`](./Preprocessing.ipynb) notebook and stored in the same folder as the neural network model using `pickle`. We simply reload it from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "\n",
    "model_dir = os.path.dirname(environ[\"INPUT_MODEL\"])\n",
    "\n",
    "if environ['MODEL_VARIANT'] != '': # remove from path because those files are shared among variants\n",
    "    preprocessing_file = preprocessing_file.replace(f\"/{environ['MODEL_VARIANT']}\",'')\n",
    "preprocessing_file = os.path.join(model_dir, \"tX.pkl\")\n",
    "\n",
    "with open(preprocessing_file, 'rb') as f:\n",
    "    preprocessing_step = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "The model is loaded using the keras APIs, and summarized below for completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Loading model from /mlinfn/shared/lamarr/scapelli/trained_models/j100/models/acceptance"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 12)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          1664        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 128)          0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          16512       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 128)          0           ['add[0][0]',                    \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          16512       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 128)          0           ['add_1[0][0]',                  \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          16512       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 128)          0           ['add_2[0][0]',                  \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          16512       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 128)          0           ['add_3[0][0]',                  \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            129         ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84,353\n",
      "Trainable params: 84,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "display (HTML(f\"Loading model from {model_dir}\"))\n",
    "\n",
    "acceptance_model = tf.keras.models.load_model(model_dir)\n",
    "acceptance_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data and evaluate the model\n",
    "\n",
    "In the next block we are defining the pipeline for loading data in chunks, apply to each chunk the inverted preprocessing step, obtain the neural network response for the entries of that data chunk, and finally upload the datachunk to GPU memory.\n",
    "\n",
    "Loading data in chunks is a task performed by the `FeatherReader` object.\n",
    "Then we apply a custom function, `my_processed_batch`, to each chunk. Such a function is obtained as a specialization of a more general function named `process_batch`, by specifying the list of feature and label names and the preprocessing step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://439de4e6-5a3e-4481-9ab3-68f845861d2c/assets\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `process_batch`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nInternalError()\n\nTraceback:\n---------\n  File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 195, in raise_on_meta_error\n    yield\n  File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6562, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/tmp/ipykernel_1122/3612913776.py\", line 7, in process_batch\n    batch[[f'predicted_{y}' for y in labels]] = acceptance_model.predict(pX, batch_size=len(pX), verbose=False)\n  File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/envs/root/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/utils.py:195\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py:6562\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[0;32m-> 6562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_extract_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch, preprocessor, features, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m batch[features] \u001b[38;5;241m=\u001b[39m invert_column_transformer(preprocessor, pX)\n\u001b[0;32m----> 7\u001b[0m batch[[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m labels]] \u001b[38;5;241m=\u001b[39m \u001b[43macceptance_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'model/dense/MatMul' defined at (most recent call last):\n    File \"/envs/root/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/envs/root/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/envs/root/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/envs/root/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/envs/root/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/envs/root/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/envs/root/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/envs/root/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/envs/root/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1122/3612913776.py\", line 22, in <module>\n      ddf\n    File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6632, in map_partitions\n      meta = _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\n    File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6743, in _get_meta_map_partitions\n      meta = _emulate(func, *args, udf=True, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6562, in _emulate\n      return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n    File \"/tmp/ipykernel_1122/3612913776.py\", line 7, in process_batch\n      batch[[f'predicted_{y}' for y in labels]] = acceptance_model.predict(pX, batch_size=len(pX), verbose=False)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 2033, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 1845, in predict_function\n      return step_function(self, iterator)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 1834, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 1823, in run_step\n      outputs = model.predict_step(data)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 1791, in predict_step\n      return self(x, training=False)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/envs/root/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 221, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'model/dense/MatMul'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model/dense/MatMul}}]] [Op:__inference_predict_function_2892]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m my_process_batch \u001b[38;5;241m=\u001b[39m partial(process_batch, \n\u001b[1;32m     13\u001b[0m                         features\u001b[38;5;241m=\u001b[39mdata_reader\u001b[38;5;241m.\u001b[39mfeatures, \n\u001b[1;32m     14\u001b[0m                         labels\u001b[38;5;241m=\u001b[39mdata_reader\u001b[38;5;241m.\u001b[39mlabels, \n\u001b[1;32m     15\u001b[0m                         preprocessor\u001b[38;5;241m=\u001b[39mpreprocessing_step\n\u001b[1;32m     16\u001b[0m                        )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## You may need to install dask-cudf\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#mamba install -n base dask-cudf -y -c rapidsai -c conda-forge -c nvidia/label/cuda-11.7.0\u001b[39;00m\n\u001b[1;32m     21\u001b[0m cdf \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mddf\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_process_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mmap_partitions(cudf\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_pandas)\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m cdf\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py:6632\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6625\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   6626\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt want the partitions to be aligned, and are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `map_partitions` directly, pass `align_dataframes=False`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6628\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   6630\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, _Frame)]\n\u001b[0;32m-> 6632\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43m_get_meta_map_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Scalar) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[1;32m   6634\u001b[0m     layer \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   6635\u001b[0m         (name, \u001b[38;5;241m0\u001b[39m): (\n\u001b[1;32m   6636\u001b[0m             apply,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6640\u001b[0m         )\n\u001b[1;32m   6641\u001b[0m     }\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py:6743\u001b[0m, in \u001b[0;36m_get_meta_map_partitions\u001b[0;34m(args, dfs, func, kwargs, meta, parent_meta)\u001b[0m\n\u001b[1;32m   6739\u001b[0m     parent_meta \u001b[38;5;241m=\u001b[39m dfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_meta\n\u001b[1;32m   6740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m no_default:\n\u001b[1;32m   6741\u001b[0m     \u001b[38;5;66;03m# Use non-normalized kwargs here, as we want the real values (not\u001b[39;00m\n\u001b[1;32m   6742\u001b[0m     \u001b[38;5;66;03m# delayed values)\u001b[39;00m\n\u001b[0;32m-> 6743\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43m_emulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mudf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6744\u001b[0m     meta_is_emulated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   6745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py:6562\u001b[0m, in \u001b[0;36m_emulate\u001b[0;34m(func, udf, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6558\u001b[0m \u001b[38;5;124;03mApply a function using args / kwargs. If arguments contain dd.DataFrame /\u001b[39;00m\n\u001b[1;32m   6559\u001b[0m \u001b[38;5;124;03mdd.Series, using internal cache (``_meta``) for calculation\u001b[39;00m\n\u001b[1;32m   6560\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(func), udf\u001b[38;5;241m=\u001b[39mudf), check_numeric_only_deprecation():\n\u001b[0;32m-> 6562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m_extract_meta(args, \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_extract_meta(kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    135\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/envs/root/lib/python3.9/site-packages/dask/dataframe/utils.py:216\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[0;34m(funcname, udf)\u001b[0m\n\u001b[1;32m    207\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    215\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Metadata inference failed in `process_batch`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nInternalError()\n\nTraceback:\n---------\n  File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 195, in raise_on_meta_error\n    yield\n  File \"/envs/root/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6562, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/tmp/ipykernel_1122/3612913776.py\", line 7, in process_batch\n    batch[[f'predicted_{y}' for y in labels]] = acceptance_model.predict(pX, batch_size=len(pX), verbose=False)\n  File \"/envs/root/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/envs/root/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n"
     ]
    }
   ],
   "source": [
    "from validation_utils import invert_column_transformer\n",
    "\n",
    "def process_batch(batch, preprocessor, features, labels):\n",
    "    pX = batch[features].values  \n",
    "    batch[features] = invert_column_transformer(preprocessor, pX)\n",
    "\n",
    "    batch[[f'predicted_{y}' for y in labels]] = acceptance_model.predict(pX, batch_size=len(pX), verbose=False)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "from functools import partial\n",
    "my_process_batch = partial(process_batch, \n",
    "                        features=data_reader.features, \n",
    "                        labels=data_reader.labels, \n",
    "                        preprocessor=preprocessing_step\n",
    "                       )\n",
    "\n",
    "## You may need to install dask-cudf\n",
    "#mamba install -n base dask-cudf -y -c rapidsai -c conda-forge -c nvidia/label/cuda-11.7.0\n",
    "\n",
    "cdf = (\n",
    "    ddf\n",
    "    .map_partitions(my_process_batch, test_dataset)\n",
    "    .map_partitions(cudf.DataFrame.from_pandas)\n",
    ")\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check, repeated\n",
    "\n",
    "To ensure everything was correct in the loading and evaluation of the model, we repeat the comparison on the distribution of labels and predictions. The distributions are qualitatively comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,2))\n",
    "df = cdf.head(1_000_000, npartitions=-1)\n",
    "plt.hist(df.acceptance, bins=np.linspace(0, 1, 101), label=\"In Acceptance (test sample)\")\n",
    "plt.hist(df.predicted_acceptance, bins=np.linspace(0, 1, 101), label=\"In Acceptance (Model)\", histtype='step', linewidth=2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"In acceptance\")\n",
    "plt.ylabel(\"Number of particles\")\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding derived variables to the dataset\n",
    "\n",
    "To build the histograms in kinematic bins we need may need to add some variable.\n",
    "Using `dask` and `cudf`, the computation of these new variables is *lazy*: it is delayed to the time when the result is read.\n",
    "At that point, the chunks are iteratively loaded from disk and the whole sequence of operations is performed on each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_momentum(df):\n",
    "    df['mc_p'] = 10**df['mc_log10_p']\n",
    "    df['mc_pz'] = (df.mc_p**2 / (1 + df.mc_tx**2 + df.mc_ty**2))**0.5\n",
    "    df['mc_px'] = df.mc_pz * df.mc_tx\n",
    "    df['mc_py'] = df.mc_pz * df.mc_ty\n",
    "    df['mc_pt'] = (df.mc_px**2 + df.mc_py**2)**0.5\n",
    "    return df\n",
    "\n",
    "def compute_eta(df):    \n",
    "    mc_theta = cp.arcsin(df.mc_pt/df.mc_p)\n",
    "    df['mc_eta'] = -cp.log(cp.tan(mc_theta/2))\n",
    "    return df\n",
    "\n",
    "def compute_phi(df):\n",
    "    df['mc_phi'] = cp.arctan2(df.mc_py, df.mc_px)\n",
    "    return df\n",
    "    \n",
    "\n",
    "cdf = (cdf.map_partitions(compute_momentum)\n",
    "      #  .map_partitions(compute_eta)\n",
    "      #  .map_partitions(compute_phi)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance histograms in momentum and pseudorapidity bins\n",
    "\n",
    "Here we report the comparison of the distribution of events:\n",
    "<UL>\n",
    "<LI> <SPAN style=\"color: blue;\" > Generated (without cuts or requirements) </SPAN>\n",
    "<LI> <SPAN style=\"color: orange;\" > Selected, or \"in acceptance (true)\" <SPAN>\n",
    "<LI> <SPAN style=\"color: green;\" > Weighted, or \"in acceptance (predicted)\" </SPAN>\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_boundaries = [500, 3_000, 5_000, 10_000, 30_000, 100_000, 200_000]\n",
    "\n",
    "retain = ['acceptance', 'predicted_acceptance', 'mc_eta', 'mc_log10_p']\n",
    "\n",
    "for part in ['h', 'mu', 'e']:\n",
    "    part_df = cdf.query(f\"mc_is_{part} == 1 and mc_eta > 0 and mc_eta < 12\")[retain].compute()\n",
    "    \n",
    "    plt.figure(figsize=(20,3))\n",
    "    for iPlot, (p_min, p_max) in enumerate(zip(p_boundaries[:-1], p_boundaries[1:]), 1):\n",
    "        plt.subplot(1, len(p_boundaries)-1, iPlot)\n",
    "        bin_df = part_df.query(f\"mc_log10_p > {np.log10(p_min)} and mc_log10_p < {np.log10(p_max)}\")\n",
    "        denominator, bins = cp.histogram(bin_df.mc_eta.values, bins=np.linspace(0, 10, 101))\n",
    "        true_numerator, _ = cp.histogram(bin_df.query('acceptance==1').mc_eta.values, bins=bins)\n",
    "        pred_numerator,_ = cp.histogram(bin_df.mc_eta.values, weights=bin_df.predicted_acceptance.values, bins=bins)\n",
    "    \n",
    "        bins = bins.get()\n",
    "        x = (bins[:-1] + bins[1:])/2\n",
    "        plt.hist(x, bins=bins, weights=denominator.get(), histtype='step', linewidth=2, alpha=0.5, label=\"Generated\")\n",
    "        plt.hist(x, bins=bins, weights=true_numerator.get(), histtype='step', linewidth=2, label=\"In acceptance (true)\")\n",
    "        plt.hist(x, bins=bins, weights=pred_numerator.get(), histtype='step', linewidth=2, label=\"In acceptance (model)\")\n",
    "        plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "        part_name = dict(h=\"Hadrons\", mu='Muons', e='Electrons')[part]\n",
    "        plt.title(f\"Acceptance for $p \\in$ [{p_min/1e3:.1f}, {p_max/1e3:.1f}] GeV/$c$\")\n",
    "        plt.xlabel(\"Pseudorapidity $\\eta$\")\n",
    "        plt.ylabel(f\"Number of $\\mathbf{{{part_name}}}$\")\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = acceptance_model\n",
    "def plot_weight_distributions(model,title=\"\"):\n",
    "    dense_layers = [layer for layer in model.layers if isinstance(layer, tf.keras.layers.Dense)]\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    colors = cmap(np.linspace(0, 1, len(dense_layers)))\n",
    "    ax = plt.axes()\n",
    " \n",
    "    # Setting the background color of the plot\n",
    "    # using set_facecolor() method\n",
    "    ax.set_facecolor(\"darkgrey\")\n",
    "    \n",
    "    for i, layer in enumerate(dense_layers):\n",
    "        weights = layer.get_weights()[0].flatten()  # Flatten the weights\n",
    "        label = f\"{layer.__class__.__name__} {i+1}, #w: {weights.shape[0]}\"\n",
    "        plt.hist(weights, density=True, bins=\"auto\", histtype='step', color=colors[i], label=label, facecolor='black')\n",
    "\n",
    "    if title != \"\":\n",
    "        plt.title(title)\n",
    "    plt.xlim(-2,2)\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have a trained model named 'model'\n",
    "plot_weight_distributions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceptance histograms in momentum and polar angle bins\n",
    "\n",
    "Here we report the comparison of the distribution of events:\n",
    "<UL>\n",
    "<LI> <SPAN style=\"color: blue;\" > Generated (without cuts or requirements) </SPAN>\n",
    "<LI> <SPAN style=\"color: orange;\" > Selected, or \"in acceptance (true)\" <SPAN>\n",
    "<LI> <SPAN style=\"color: green;\" > Weighted, or \"in acceptance (predicted)\" </SPAN>\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m retain \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceptance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_acceptance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_phi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc_log10_p\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     part_df \u001b[38;5;241m=\u001b[39m \u001b[43mcdf\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmc_is_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m == 1 and mc_phi > -3.5 and mc_phi < 3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)[retain]\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m iPlot, (p_min, p_max) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(p_boundaries[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], p_boundaries[\u001b[38;5;241m1\u001b[39m:]), \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cdf' is not defined"
     ]
    }
   ],
   "source": [
    "p_boundaries = [500, 3_000, 5_000, 10_000, 30_000, 100_000, 200_000]\n",
    "\n",
    "retain = ['acceptance', 'predicted_acceptance', 'mc_phi', 'mc_log10_p']\n",
    "\n",
    "for part in ['h', 'mu', 'e']:\n",
    "    part_df = cdf.query(f\"mc_is_{part} == 1 and mc_phi > -3.5 and mc_phi < 3.5\")[retain].compute()\n",
    "    \n",
    "    plt.figure(figsize=(20,3))\n",
    "    for iPlot, (p_min, p_max) in enumerate(zip(p_boundaries[:-1], p_boundaries[1:]), 1):\n",
    "        plt.subplot(1, len(p_boundaries)-1, iPlot)\n",
    "        bin_df = part_df.query(f\"mc_log10_p > {np.log10(p_min)} and mc_log10_p < {np.log10(p_max)}\")\n",
    "        denominator, bins = cp.histogram(bin_df.mc_phi.values, bins=np.linspace(-np.pi, np.pi, 51))\n",
    "        true_numerator, _ = cp.histogram(bin_df.query('acceptance==1').mc_phi.values, bins=bins)\n",
    "        pred_numerator,_ = cp.histogram(bin_df.mc_phi.values, weights=bin_df.predicted_acceptance.values, bins=bins)\n",
    "    \n",
    "        bins = bins.get()\n",
    "        x = (bins[:-1] + bins[1:])/2\n",
    "        plt.hist(x, bins=bins, weights=denominator.get(), histtype='step', linewidth=2, alpha=0.5, label=\"Generated\")\n",
    "        plt.hist(x, bins=bins, weights=true_numerator.get(), histtype='step', linewidth=2, label=\"In acceptance (true)\")\n",
    "        plt.hist(x, bins=bins, weights=pred_numerator.get(), histtype='step', linewidth=2, label=\"In acceptance (model)\")\n",
    "        plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "        part_name = dict(h=\"Hadrons\", mu='Muons', e='Electrons')[part]\n",
    "        plt.title(f\"Acceptance for $p \\in$ [{p_min/1e3:.1f}, {p_max/1e3:.1f}] GeV/$c$\")\n",
    "        plt.xlabel(\"Azimuthal angle $\\phi$\")\n",
    "        plt.ylabel(f\"Number of $\\mathbf{{{part_name}}}$\")\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "In this notebook we produced some plots showing the agreement between the acceptance obtained from detailed simulation and the modeled acceptance probability.\n",
    "\n",
    "Additional comparisons and plots might be added in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Acceptance-validation.ipynb as html,pdf in /workarea/local/shared/scapelli/notebooks_exports/j100/v1\n"
     ]
    }
   ],
   "source": [
    "### export notebooks for comparisons\n",
    "if environ.get('NB_EXPORT',\"False\")==\"True\":\n",
    "    from os import makedirs,system\n",
    "    \n",
    "    nb_save  = f\"{environ['HOME_DIR']}/notebooks_exports\"  # export output dir\n",
    "    nb_save  = nb_save+environ['TRAINING_DATA_FOLDER']     # according to train data\n",
    "    if environ['MODEL_VARIANT'] != '':\n",
    "        nb_save  = nb_save+'/'+environ['MODEL_VARIANT']    # according to model variant\n",
    "    makedirs(nb_save,exist_ok=True)                        \n",
    "    \n",
    "    nbs_path = f\"{environ['HOME_DIR']}/lb-trksim-train/notebooks\" # notebooks folder\n",
    "    nb_filename = \"Acceptance-validation.ipynb\"                   # notebook name\n",
    "    extensions  = [\"html\",\"pdf\"]                                  # export formats\n",
    "    for ext in extensions:\n",
    "        system(\"jupyter nbconvert --log-level=40 --no-input --output-dir {0} --to {1} {2}/{3}\".format(nb_save,ext.upper(),nbs_path,nb_filename))\n",
    "    print(\"Exported {} as {} in {}\".format(nb_filename,','.join(extensions),nb_save))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HEP",
   "language": "python",
   "name": "hep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
